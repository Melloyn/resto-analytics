import pandas as pd
import numpy as np
import re
import os
import requests
from io import BytesIO
from datetime import datetime

# --- CONSTANTS ---
IGNORE_NAMES = [
    "–ë–∞—Ä –ú–µ—Å—Ç–æ", "–ë–∞—Ä –ú–µ—Å—Ç–æ –ë—É—Ä–≥–µ—Ä–Ω–∞—è", "–ò—Ç–æ–≥–æ", "–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞", "–°–∫–ª–∞–¥—ã", 
    "–ù–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ", "–¢–æ–≤–∞—Ä—ã", "–£—Å–ª—É–≥–∏", "–ï–ì–ê–ò–°", "–ê–ª–∫–æ–≥–æ–ª—å",
    "–ü–∏–≤–æ —Ä–∞–∑–ª–∏–≤–Ω–æ–µ –†–æ—Å—Å–∏—è", "–ü–∏–≤–æ –∏–º–ø–æ—Ä—Ç–Ω–æ–µ", "–ü–∏–≤–æ –±—É—Ç—ã–ª–æ—á–Ω–æ–µ", "–°–∏–¥—Ä", 
    "–í–æ–¥–∫–∞", "–°–∞–º–æ–≥–æ–Ω", "–ù–∞—Å—Ç–æ–π–∫–∏", "–ß–∞—á–∞/–ì—Ä–∞–ø–∞", "–î–∂–∏–Ω", "–í–∏—Å–∫–∏/–ë—É—Ä–±–æ–Ω", 
    "–¢–µ–∫–∏–ª–∞", "–†–æ–º", "–ö–æ–Ω—å—è–∫/–ë—Ä–µ–Ω–¥–∏", "–ê–ø–µ—Ä–∏—Ç–∏–≤—ã", "–õ–∏–∫–µ—Ä—ã –∏ –Ω–∞—Å—Ç–æ–π–∫–∏", 
    "–í–µ—Ä–º—É—Ç—ã", "–ò–≥—Ä–∏—Å—Ç—ã–µ –≤–∏–Ω–∞", "–¢–∏—Ö–∏–µ –±–µ–ª—ã–µ –≤–∏–Ω–∞", "–¢–∏—Ö–∏–µ —Ä–æ–∑–æ–≤—ã–µ –≤–∏–Ω–∞", 
    "–¢–∏—Ö–∏–µ –∫—Ä–∞—Å–Ω—ã–µ –≤–∏–Ω–∞", "–ö—Ä–µ–ø–ª–µ–Ω—ã–µ –≤–∏–Ω–∞", "–ë/–∞ –Ω–∞–ø–∏—Ç–∫–∏", "–ö–æ–∫—Ç–µ–π–ª–∏ –ø–æ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É"
]

RUS_MONTHS = {
    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5, '–∏—é–Ω—è': 6,
    '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12,
    '—è–Ω–≤': 1, '—Ñ–µ–≤': 2, '–º–∞—Ä': 3, '–∞–ø—Ä': 4, '–º–∞–π': 5, '–∏—é–Ω': 6,
    '–∏—é–ª': 7, '–∞–≤–≥': 8, '—Å–µ–Ω': 9, '–æ–∫—Ç': 10, '–Ω–æ—è': 11, '–¥–µ–∫': 12
}

RUS_MONTH_NAMES = {
    1: '–Ø–Ω–≤–∞—Ä—å', 2: '–§–µ–≤—Ä–∞–ª—å', 3: '–ú–∞—Ä—Ç', 4: '–ê–ø—Ä–µ–ª—å', 5: '–ú–∞–π', 6: '–ò—é–Ω—å',
    7: '–ò—é–ª—å', 8: '–ê–≤–≥—É—Å—Ç', 9: '–°–µ–Ω—Ç—è–±—Ä—å', 10: '–û–∫—Ç—è–±—Ä—å', 11: '–ù–æ—è–±—Ä—å', 12: '–î–µ–∫–∞–±—Ä—å'
}

CACHE_FILE = "data_cache.parquet"
LAST_SYNC_META = {
    "dropped_stats": {"count": 0, "cost": 0.0, "items": []},
    "warnings": [],
}

# --- HELPERS ---
def parse_russian_date(text):
    if not isinstance(text, str): return None
    text = text.lower()
    match_text = re.search(r'(\d{1,2})\s+([–∞-—è]+)\s+(\d{4})', text)
    if match_text:
        day, month_str, year = match_text.groups()
        if month_str in RUS_MONTHS:
            try:
                return datetime(int(year), RUS_MONTHS[month_str], int(day))
            except ValueError: return None
    match_digit = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', text)
    if match_digit:
        try:
            return datetime.strptime(match_digit.group(0), '%d.%m.%Y')
        except ValueError: return None
    return None

def detect_header_row(df_preview, required_column):
    for idx in range(min(20, len(df_preview))):
        row_values = df_preview.iloc[idx].astype(str).str.lower()
        if row_values.str.contains(required_column.lower(), regex=False).any():
            return idx
    return None

from services import category_service

def get_macro_category(cat):
    if cat in ['‚òï –ö–æ—Ñ–µ', 'üçµ –ß–∞–π', 'üçì –ú–∏–ª–∫/–§—Ä–µ—à/–°–º—É–∑–∏', 'üßâ –ö–æ–∫—Ç–µ–π–ª—å –ë/–ê', 'üö∞ –†–æ–∑–ª–∏–≤ –ë/–ê', 'ü•§ –°—Ç–µ–∫–ª–æ/–ë–∞–Ω–∫–∞ –ë/–ê']: 
        return '‚òï –ë–µ–∑–∞–ª–∫–æ–≥–æ–ª—å–Ω–æ–µ'
    if cat in ['üçè –°–∏–¥—Ä –®–¢', 'üçæ –ü–∏–≤–æ –®–¢', 'üç∫ –ü–∏–≤–æ –†–æ–∑–ª–∏–≤']: 
        return 'üç∫ –ü–∏–≤–æ/–°–∏–¥—Ä'
    if cat in ['ü•É –í–∏—Å–∫–∏', 'üíß –í–æ–¥–∫–∞', 'üè¥‚Äç‚ò†Ô∏è –†–æ–º', 'üåµ –¢–µ–∫–∏–ª–∞', 'üå≤ –î–∂–∏–Ω', 'üçá –ö–æ–Ω—å—è–∫/–ë—Ä–µ–Ω–¥–∏', 'üçí –õ–∏–∫–µ—Ä/–ù–∞—Å—Ç–æ–π–∫–∞']: 
        return 'ü•É –ö—Ä–µ–ø–∫–æ–µ'
    return cat

def detect_category_granular(name_input, mapping=None):
    name = str(name_input).strip().lower()
    
    # 1. DYNAMIC MAPPING (JSON)
    # Check exact match first
    # mapping keys might be original case, so we should check carefully
    # Assuming mapping keys are case-sensitive or we lower them?
    # Let's assume exact match for now as per `admin_view` editor
    if mapping:
        if name in mapping:
            return mapping[name]
        if name_input in mapping:
            return mapping[name_input]
    
    # 2. HARDCODED FALLBACK (Removed)

    # –†–ï–ó–ï–†–í–ù–´–ô –ü–û–ò–°–ö
    food_keywords = ['–±—É—Ä–≥–µ—Ä', '—Å—É–ø', '—Å–∞–ª–∞—Ç', '—Ñ—Ä–∏', '—Å—ã—Ä', '–º—è—Å–æ', '—Å—Ç–µ–π–∫', '—Ö–ª–µ–±', '—Å–æ—É—Å', '–∫–∞—Ä—Ç–æ—Ñ–µ–ª—å', '–≥—Ä–µ–Ω–∫–∏', '–∫—Ä—ã–ª—å—è', '–∫—Ä–µ–≤–µ—Ç–∫–∏', '–ø–∞—Å—Ç–∞', '—Å—É—Ö–∞—Ä–∏–∫–∏', '—Å—ç–Ω–¥–≤–∏—á', '–¥–æ–±–∞–≤–∫–∞', '–¥–µ—Å–µ—Ä—Ç', '–º–æ—Ä–æ–∂–µ–Ω–æ–µ', '—á–∏–∑–∫–µ–π–∫', '–Ω–∞—á–æ—Å', '–∫–µ—Å–∞–¥–∏–ª—å—è']
    if any(w in name for w in food_keywords): return 'üçî –ï–¥–∞ (–ö—É—Ö–Ω—è)'

    extra_keywords = ['—Å–∏—Ä–æ–ø', '–¥–æ–ø.', '—Å–ª–∏–≤–∫–∏', '–º–æ–ª–æ–∫–æ 50', '–ª–∏–º–æ–Ω 20', '–ª–∞–π–º 20', '–º—è—Ç–∞ 20', '–∞–ø–µ–ª—å—Å–∏–Ω 20', '–º—ë–¥']
    if any(w in name for w in extra_keywords): return 'üç¨ –î–æ–ø. –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã'

    if any(w in name for w in ['–∫–æ—Ñ–µ', '–∫–∞–ø—É—á–∏–Ω–æ', '–ª–∞—Ç—Ç–µ', '—ç—Å–ø—Ä–µ—Å—Å–æ', '–∞–º–µ—Ä–∏–∫–∞–Ω–æ', '—Ä–∞—Ñ', '—Ñ–ª—ç—Ç —É–∞–π—Ç']): return '‚òï –ö–æ—Ñ–µ'
    if any(w in name for w in ['—á–∞–π', '—Å–µ–Ω—á–∞', '–ø—É—ç—Ä', '—ç—Ä–ª –≥—Ä–µ–π']): return 'üçµ –ß–∞–π'
    if any(w in name for w in ['—Å–º—É–∑–∏', '–º–∏–ª–∫', '—à–µ–π–∫', '—Ñ—Ä–µ—à']): return 'üçì –ú–∏–ª–∫/–§—Ä–µ—à/–°–º—É–∑–∏'
    if '–±/–∞' in name and any(w in name for w in ['–º–æ—Ö–∏—Ç–æ', '–ø–∏–Ω–∞', '–≥–ª–∏–Ω—Ç–≤–µ–π–Ω', '–∫–æ–∫—Ç–µ–π–ª—å']): return 'üßâ –ö–æ–∫—Ç–µ–π–ª—å –ë/–ê'
    if any(w in name for w in ['–º–æ—Ä—Å', '–ª–∏–º–æ–Ω–∞–¥', '–Ω–∞–ø–∏—Ç–æ–∫']): 
        if not any(b in name for b in ['—á–µ—Ä–Ω–æ–≥–æ–ª–æ–≤–∫–∞', '–Ω–∞—Ç–∞—Ö—Ç–∞—Ä–∏']): return 'üö∞ –†–æ–∑–ª–∏–≤ –ë/–ê'
    if any(w in name for w in ['cola', '—Ç–æ–Ω–∏–∫', 'red bull', 'rich', '–≤–æ–¥–∞', 'water', '–∫–æ–ª–∞']): return 'ü•§ –°—Ç–µ–∫–ª–æ/–ë–∞–Ω–∫–∞ –ë/–ê'

    if '—Å–∏–¥—Ä' in name: return 'üçè –°–∏–¥—Ä –®–¢'
    if any(w in name for w in ['corona', 'clausthaler']) or ('–ø–∏–≤–æ' in name and '—à—Ç' in name): return 'üçæ –ü–∏–≤–æ –®–¢'
    if any(w in name for w in ['–ø–∏–≤–æ', 'beer', 'ale', 'lager', 'stout', '—Å–≤–µ—Ç–ª–æ–µ', '—Ç–µ–º–Ω–æ–µ']): return 'üç∫ –ü–∏–≤–æ –†–æ–∑–ª–∏–≤'
    if any(w in name for w in ['–≤–∏—Å–∫–∏', 'jameson', 'jack', 'jim beam', 'macallan']): return 'ü•É –í–∏—Å–∫–∏'
    if any(w in name for w in ['–≤–æ–¥–∫–∞', '–±–µ–ª—É–≥–∞', '—Ö–∞—Å–∫–∏', '–æ–Ω–µ–≥–∏–Ω', 'finlandia']): return 'üíß –í–æ–¥–∫–∞'
    if any(w in name for w in ['—Ä–æ–º', 'bacardi', 'morgan', 'havana']): return 'üè¥‚Äç‚ò†Ô∏è –†–æ–º'
    if any(w in name for w in ['—Ç–µ–∫–∏–ª–∞', 'olmeca', 'espolon']): return 'üåµ –¢–µ–∫–∏–ª–∞'
    if any(w in name for w in ['–¥–∂–∏–Ω', 'beefeater', 'gordon', 'bombay']): return 'üå≤ –î–∂–∏–Ω'
    if any(w in name for w in ['–∫–æ–Ω—å—è–∫', '–∞—Ä–∞—Ä–∞—Ç', 'hennessy']): return 'üçá –ö–æ–Ω—å—è–∫/–ë—Ä–µ–Ω–¥–∏'
    if any(w in name for w in ['–ª–∏–∫–µ—Ä', '–Ω–∞—Å—Ç–æ–π–∫–∞', '–µ–≥–µ—Ä—å', 'baileys', '–∞–ø–µ—Ä–æ–ª—å', '—Å–∞–º–±—É–∫–∞']): return 'üçí –õ–∏–∫–µ—Ä/–ù–∞—Å—Ç–æ–π–∫–∞'
    if any(w in name for w in ['–≤–∏–Ω–æ', 'wine', '–±—Ä—é—Ç', '–ø—Ä–æ—Å–µ–∫–∫–æ', '—à–∞—Ä–¥–æ–Ω–µ']): return 'üç∑ –í–∏–Ω–æ'
    if any(w in name for w in ['–∫–æ–∫—Ç–µ–π–ª—å', '—à–æ—Ç', '–ª–æ–Ω–≥', '–¥–∞–π–∫–∏—Ä–∏', '–º–∞—Ä–≥–∞—Ä–∏—Ç–∞']): return 'üçπ –ö–æ–∫—Ç–µ–π–ª–∏'

    return 'üì¶ –ü—Ä–æ—á–µ–µ'

def apply_categories(df):
    if df is None or df.empty or "–ë–ª—é–¥–æ" not in df.columns:
        return df
    cat_mapping = category_service.load_categories()
    df = df.copy()
    df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] = df['–ë–ª—é–¥–æ'].apply(lambda x: detect_category_granular(x, cat_mapping))
    df['–ú–∞–∫—Ä–æ_–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] = df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].apply(get_macro_category)
    return df

# --- CORE PARSING ---
def process_single_file(file_content, filename=""):
    """
    Parses a single Excel/CSV file into a DataFrame.
    Returns: (DataFrame, error_message, warnings, dropped_stats)
    dropped_stats is a dict: {'count': int, 'cost': float, 'items': list}
    """
    warnings = []
    dropped_stats = {'count': 0, 'cost': 0.0, 'items': []}
    
    try:
        # 1. READ RAW (Snippet for header detection)
        if isinstance(file_content, BytesIO):
             file_content.seek(0)
             content_for_preview = BytesIO(file_content.read())
             file_content.seek(0) # Reset main pointer
        else:
             # If it's a file path, read bytes
             with open(file_content, 'rb') as f:
                 raw_bytes = f.read()
             file_content = BytesIO(raw_bytes)
             content_for_preview = BytesIO(raw_bytes)

        try:
            df_raw = pd.read_csv(content_for_preview, header=None, nrows=20, sep=None, engine='python')
        except:
            content_for_preview.seek(0)
            df_raw = pd.read_excel(content_for_preview, header=None, nrows=20)

        # 2. DETECT DATE
        header_text = " ".join(df_raw.iloc[0:10, 0].astype(str).tolist())
        report_date = parse_russian_date(header_text)

        if not report_date:
            month_map = {'jan': '—è–Ω–≤–∞—Ä—è', 'feb': '—Ñ–µ–≤—Ä–∞–ª—è', 'mar': '–º–∞—Ä—Ç–∞', 'apr': '–∞–ø—Ä–µ–ª—è', 'may': '–º–∞—è', 'jun': '–∏—é–Ω—è', 'jul': '–∏—é–ª—è', 'aug': '–∞–≤–≥—É—Å—Ç–∞', 'sep': '—Å–µ–Ω—Ç—è–±—Ä—è', 'oct': '–æ–∫—Ç—è–±—Ä—è', 'nov': '–Ω–æ—è–±—Ä—è', 'dec': '–¥–µ–∫–∞–±—Ä—è'}
            for eng, rus in month_map.items():
                if eng in filename.lower():
                    d_match = re.search(r'(\d{1,2})', filename)
                    if d_match:
                        # Try to find year in filename
                        y_match = re.search(r'(20\d{2})', filename)
                        current_year = int(y_match.group(1)) if y_match else datetime.now().year
                        
                        report_date = datetime(current_year, RUS_MONTHS[rus], int(d_match.group(1)))
                        break
        
        if not report_date:
            warnings.append(f"–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: {filename}")
            return None, "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∞—Ç—É –æ—Ç—á–µ—Ç–∞", warnings, dropped_stats

        # 3. LOCATE HEADER ROW
        header_row = detect_header_row(df_raw, "–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°")
        if header_row is None:
            warnings.append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–∞ 6: {filename}")
            header_row = 5

        # 4. READ FULL DATAFRAME
        file_content.seek(0)
        try:
            df = pd.read_csv(file_content, header=header_row, sep=None, engine='python')
        except:
            file_content.seek(0)
            df = pd.read_excel(file_content, header=header_row)

        df.columns = df.columns.astype(str).str.strip()
        
        # VALIDATE COLUMNS
        required_cols = ['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            return None, f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_cols)}", warnings, dropped_stats

        # 5. CLEAN & CONVERT NUMBERS
        cols_to_num = ['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°']
        for col in cols_to_num:
            if col in df.columns:
                # Keep original for debug before conversion? No, convert first
                df[col] = df[col].astype(str).str.replace(r'\s+', '', regex=True).str.replace(',', '.')
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

        col_name = df.columns[0] # Usually "–ù–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞" or "–ë–ª—é–¥–æ"
        
        # 6. HANDLE MULTI-DAY FILES (Date markers inside first column)
        # Example marker: "01.01.2025 17:00:00" followed by item rows.
        first_col_raw = df[col_name].astype(str).str.strip()
        date_tokens = first_col_raw.str.extract(r'(?P<d>\d{2}\.\d{2}\.\d{4})', expand=True)['d']
        row_dates = pd.to_datetime(date_tokens, format='%d.%m.%Y', errors='coerce')
        unique_row_dates = row_dates.dropna().dt.normalize().nunique()

        if unique_row_dates > 1:
            df['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'] = row_dates.dt.normalize().ffill()
            # Rows containing date markers are structural rows, exclude from item lines.
            df = df[row_dates.isna()].copy()
            # Keep only rows that have a resolved day after forward fill.
            df = df[df['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'].notna()].copy()
        else:
            df['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'] = report_date

        # 7. CAPTURE DROPPED ROWS (DEBUG)
        # Identify rows that would be dropped
        # A. Empty Identifiers
        # B. Ignore Names
        # C. "–ò—Ç–æ–≥–æ" rows
        
        # We need to compute 'dropped' before we actually drop them to sum their metrics
        
        # Normalized Identifier
        df['norm_name'] = df[col_name].astype(str).str.strip()
        
        # Filter Masks
        mask_valid_name = df[col_name].notna()
        mask_not_ignore = ~df['norm_name'].isin(IGNORE_NAMES)
        mask_not_total = ~df['norm_name'].str.contains("–ò—Ç–æ–≥–æ", case=False)
        
        mask_keep = mask_valid_name & mask_not_ignore & mask_not_total
        
        # Extract Dropped Data
        df_dropped = df[~mask_keep].copy()
        if not df_dropped.empty:
            dropped_stats['count'] = len(df_dropped)
            if '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å' in df_dropped.columns:
                dropped_stats['cost'] = df_dropped['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å'].sum()
            
            # Save top 50 dropped items for review
            items_list = df_dropped[['norm_name', '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å']].to_dict('records')
            dropped_stats['items'] = items_list

        # Apply Filter
        df = df[mask_keep].copy()
        
        # 8. ENRICH DATA
        df['Unit_Cost'] = np.where(df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] != 0, df['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å'] / df['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'], 0)
        df['–§—É–¥–∫–æ—Å—Ç'] = np.where(df['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'] > 0, (df['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å'] / df['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'] * 100), 0)
        df = df.rename(columns={col_name: '–ë–ª—é–¥–æ'})
        
        # Load mapping once per file (or rely on cache)
        df = apply_categories(df)
        
        # Helper for vendor
        if '–ü–æ—Å—Ç–∞–≤—â–∏–∫' in df.columns:
            df['–ü–æ—Å—Ç–∞–≤—â–∏–∫'] = df['–ü–æ—Å—Ç–∞–≤—â–∏–∫'].fillna('–ù–µ —É–∫–∞–∑–∞–Ω')
        else:
            df['–ü–æ—Å—Ç–∞–≤—â–∏–∫'] = '–ù–µ —É–∫–∞–∑–∞–Ω'

        return df, None, warnings, dropped_stats

    except Exception as exc:
        return None, f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {exc}", warnings, dropped_stats

# --- ANALYTICS ---
def calculate_insights(df_curr, df_prev, cur_rev, prev_rev, cur_fc):
    """
    Calculates business insights/alerts based on current and previous data.
    Returns a list of dicts: {'type': str, 'message': str, 'level': str}
    Levels: 'success', 'warning', 'error', 'info'
    """
    insights = []
    
    # 1. Revenue Check
    if prev_rev > 0:
        rev_diff_pct = (cur_rev - prev_rev) / prev_rev * 100
        if rev_diff_pct < -10:
            insights.append({
                'type': 'rev_drop',
                'message': f"üìâ **–¢—Ä–µ–≤–æ–≥–∞ –ø–æ –í—ã—Ä—É—á–∫–µ**: –ü–∞–¥–µ–Ω–∏–µ –Ω–∞ {abs(rev_diff_pct):.1f}% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ—à–ª—ã–º –ø–µ—Ä–∏–æ–¥–æ–º.",
                'level': 'error'
            })
        elif rev_diff_pct > 20:
            insights.append({
                'type': 'rev_growth',
                'message': f"üöÄ **–û—Ç–ª–∏—á–Ω—ã–π —Ä–æ—Å—Ç**: –í—ã—Ä—É—á–∫–∞ –≤—ã—Ä–æ—Å–ª–∞ –Ω–∞ {rev_diff_pct:.1f}%!",
                'level': 'success'
            })

    # 2. Food Cost Check
    TARGET_FC = 35.0
    if cur_fc > TARGET_FC:
        insights.append({
            'type': 'high_fc',
            'message': f"‚ö†Ô∏è **–í—ã—Å–æ–∫–∏–π –§—É–¥-–∫–æ—Å—Ç**: –¢–µ–∫—É—â–∏–π {cur_fc:.1f}% (–¶–µ–ª—å: {TARGET_FC}%).",
            'level': 'warning'
        })
    
    # 3. Ingredient Inflation (Top Spike)
    if not df_prev.empty and 'Unit_Cost' in df_curr.columns and 'Unit_Cost' in df_prev.columns:
        # Compare average purchase prices
        curr_prices = df_curr.groupby('–ë–ª—é–¥–æ')['Unit_Cost'].mean()
        prev_prices = df_prev.groupby('–ë–ª—é–¥–æ')['Unit_Cost'].mean()
        
        safe_prev_prices = prev_prices.replace(0, np.nan)
        price_changes = (curr_prices - safe_prev_prices) / safe_prev_prices * 100
        price_changes = price_changes.replace([np.inf, -np.inf], np.nan).dropna().sort_values(ascending=False)
        
        if not price_changes.empty:
            top_inflator = price_changes.index[0]
            top_val = price_changes.iloc[0]
            if top_val > 15: # Raised/Spiked more than 15%
                insights.append({
                    'type': 'inflation',
                    'message': f"üí∏ **–°–∫–∞—á–æ–∫ —Ü–µ–Ω—ã**: {top_inflator} –ø–æ–¥–æ—Ä–æ–∂–∞–ª –Ω–∞ {top_val:.0f}%.",
                    'level': 'warning'
                })

    # 4. Dead Items ("Dogs")
    # Logic: Low Sales (< Avg) AND Low Margin (< Avg)
    if not df_curr.empty:
        item_stats = df_curr.groupby('–ë–ª—é–¥–æ').agg({'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': 'sum', '–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°': 'sum', '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å': 'sum'}).reset_index()
        item_stats['–ú–∞—Ä–∂–∞'] = item_stats['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'] - item_stats['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å']
        item_stats = item_stats[item_stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] > 0]
        
        if not item_stats.empty:
            avg_qty = item_stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'].mean()
            avg_margin = item_stats['–ú–∞—Ä–∂–∞'].mean()
            
            dogs = item_stats[(item_stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] < avg_qty * 0.5) & (item_stats['–ú–∞—Ä–∂–∞'] < avg_margin * 0.5)]
            if len(dogs) > 5:
                insights.append({
                    'type': 'dogs',
                    'message': f"üê∂ **–ú–µ—Ä—Ç–≤—ã–π –≥—Ä—É–∑**: –ù–∞–π–¥–µ–Ω–æ {len(dogs)} –ø–æ–∑–∏—Ü–∏–π '–°–æ–±–∞–∫' (–º–∞–ª–æ –ø—Ä–æ–¥–∞–∂, –º–∞–ª–æ –¥–µ–Ω–µ–≥).",
                    'level': 'info'
                })

    if not insights:
        insights.append({
            'type': 'all_good',
            'message': "‚úÖ **–í—Å—ë —Å–ø–æ–∫–æ–π–Ω–æ**: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.",
            'level': 'success'
        })

    return insights

def get_last_sync_meta():
    return LAST_SYNC_META


from services import parsing_service

# --- IN-MEMORY STORAGE FOR NEW DATA TYPES (Reset on Reload) ---
_RECIPES_DB = {} # dish_name -> [ingredients]
_STOCK_DF = None # DataFrame
_TURNOVER_HISTORY_DF = None # History DataFrame

def get_recipes_map():
    return _RECIPES_DB

def get_stock_data():
    return _STOCK_DF

def get_turnover_history():
    return _TURNOVER_HISTORY_DF

def download_and_process_yandex(yandex_token, yandex_path="RestoAnalytic"):
    if not yandex_token:
        return False, "–ù–µ –∑–∞–¥–∞–Ω —Ç–æ–∫–µ–Ω –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞."

    headers = {"Authorization": f"OAuth {yandex_token}"}
    api_url = "https://cloud-api.yandex.net/v1/disk/resources"
    data_frames = []
    dropped_total = {"count": 0, "cost": 0.0}
    dropped_items = []
    warnings_total = []
    
    # New Data Containers
    global _RECIPES_DB, _STOCK_DF, _TURNOVER_HISTORY_DF
    recipes_list = []
    stock_parts = []
    turnover_history_parts = []

    def list_items(path, limit=1000):
        items = []
        offset = 0
        while True:
            resp = requests.get(
                api_url,
                headers=headers,
                params={"path": path, "limit": limit, "offset": offset},
                timeout=20,
            )
            if resp.status_code != 200:
                return None
            page = resp.json().get("_embedded", {}).get("items", [])
            if not page:
                break
            items.extend(page)
            if len(page) < limit:
                break
            offset += limit
        return items

    def get_files_recursive(path):
        items = list_items(path)
        if items is None:
            return []
        files = [i for i in items if i.get("type") == "file"]
        dirs = [i for i in items if i.get("type") == "dir"]
        result = [f for f in files if str(f.get("name", "")).lower().endswith((".xlsx", ".csv"))]
        for d in dirs:
            result.extend(get_files_recursive(d.get("path")))
        return result

    def process_remote_file(file_meta, venue):
        file_url = file_meta.get("file")
        filename = file_meta.get("name", "")
        if not file_url:
            return
        
        # Check file type based on path or name
        path = str(file_meta.get("path", "")) # e.g. "disk:/RestoAnalytic/..."
        
        resp = requests.get(file_url, headers=headers, timeout=30)
        if resp.status_code != 200:
            warnings_total.append(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å: {filename}")
            return
            
        content = BytesIO(resp.content)
        
        # 1. Technological Maps
        if "TechnologicalMaps" in path:
            res_list, err = parsing_service.parse_ttk(content, filename)
            if err:
                warnings_total.append(f"TTK Error {filename}: {err}")
            elif res_list:
                recipes_list.extend(res_list)
            return

        # 2. Product Turnover
        # 2. Product Turnover
        if "ProductTurnover" in path:
            df_turn, df_hist, err = parsing_service.parse_turnover(content, filename)
            if err:
                warnings_total.append(f"Turnover Error {filename}: {err}")
            else:
                if df_turn is not None:
                    # Attach report timestamp from file metadata for correct "latest stock" selection
                    report_ts = file_meta.get("modified") or file_meta.get("created")
                    if report_ts:
                        df_turn = df_turn.copy()
                        df_turn["report_date"] = report_ts
                    stock_parts.append(df_turn)
                if df_hist is not None and not df_hist.empty:
                    turnover_history_parts.append(df_hist)
            return

        # 3. Sales Analysis (Default or Explicit)
        # Process ONLY if it is SalesAnalysis or root file (legacy)
        # Avoid processing other known folders if they slip through
        if "TechnologicalMaps" not in path and "ProductTurnover" not in path:
            # Explicit check for SalesAnalysis folder or logic for root files
            # For now, relying on the fact we are filtering in the calling loop or here
            # We enforce that we only process as sales if NOT the other types
            
            df, err, warns, dropped = process_single_file(content, filename=filename)
            warnings_total.extend(warns)
            dropped_total["count"] += dropped.get("count", 0)
            dropped_total["cost"] += float(dropped.get("cost", 0.0))
            dropped_items.extend(dropped.get("items", []))
            if err:
                warnings_total.append(f"{filename}: {err}")
                return
            if df is not None and not df.empty:
                df["–¢–æ—á–∫–∞"] = venue
                data_frames.append(df)

    try:
        root_items = list_items(yandex_path)
        if root_items is None:
            return False, "–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–ø–∫–µ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ."
        
        # Files in root
        root_files = [
            i for i in root_items
            if i.get("type") == "file" and str(i.get("name", "")).lower().endswith((".xlsx", ".csv"))
        ]
        
        # Folders
        subfolders = [i for i in root_items if i.get("type") == "dir"]

        # Process Root Files (Assume Sales Data for backward compatibility)
        for f in root_files:
            process_remote_file(f, "Mesto")
            
        # Process Folders
        for folder in subfolders:
            venue = folder.get("name", "Unknown")
            # Note: We need to handle the space in " TechnologicalMaps" if it exists on disk
            # get_files_recursive uses the 'path' property which is URL encoded or exact path
            # so it should work regardless of local name display
            
            for f in get_files_recursive(folder.get("path")):
                # Filter Logic is now moved inside process_remote_file for cleaner separation
                process_remote_file(f, venue)

        # AGGREGATE RESULTS
        
        # 1. Sales
        if not data_frames and not recipes_list and not stock_parts:
             return False, "–§–∞–π–ª—ã –Ω–∞–π–¥–µ–Ω—ã, –Ω–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã."

        if data_frames:
            full_df = pd.concat(data_frames, ignore_index=True)
            if "–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞" in full_df.columns:
                full_df["–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞"] = pd.to_datetime(full_df["–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞"], errors="coerce")
                full_df = full_df.dropna(subset=["–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞"]).sort_values("–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞")
            full_df.to_parquet(CACHE_FILE, index=False)
        else:
            # If no sales data but we have other data, we strictly shouldn't fail broadly
            # But specific to existing logic, cache file might be needed
            pass

        # 2. Recipes
        _RECIPES_DB = {}
        for r in recipes_list:
            _RECIPES_DB[r['dish_name']] = r['ingredients']
            
        # 3. Stock
        if stock_parts:
            _STOCK_DF = pd.concat(stock_parts, ignore_index=True)
            # If multiple turnover files exist, use latest stock per ingredient (not sum)
            if "report_date" in _STOCK_DF.columns:
                _STOCK_DF["report_date"] = pd.to_datetime(_STOCK_DF["report_date"], errors="coerce")
                _STOCK_DF = _STOCK_DF.sort_values("report_date")
                _STOCK_DF = _STOCK_DF.groupby("ingredient", as_index=False).last()
            else:
                _STOCK_DF = _STOCK_DF.groupby("ingredient", as_index=False).last()
        else:
            _STOCK_DF = None
            
        # 4. History
        if turnover_history_parts:
            _TURNOVER_HISTORY_DF = pd.concat(turnover_history_parts, ignore_index=True)
            _TURNOVER_HISTORY_DF = _TURNOVER_HISTORY_DF.drop_duplicates()
        else:
            _TURNOVER_HISTORY_DF = None

        dropped_df = pd.DataFrame(dropped_items)
        if not dropped_df.empty and "–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å" in dropped_df.columns:
            dropped_df = dropped_df.sort_values(by="–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å", ascending=False)
            dropped_top = dropped_df.head(50).to_dict("records")
        else:
            dropped_top = dropped_items[:50]

        LAST_SYNC_META["dropped_stats"] = {
            "count": int(dropped_total["count"]),
            "cost": float(dropped_total["cost"]),
            "items": dropped_top,
        }
        LAST_SYNC_META["warnings"] = warnings_total

        msg = f"–û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ –ø—Ä–æ–¥–∞–∂: {len(full_df) if data_frames else 0}. "
        msg += f"–†–µ—Ü–µ–ø—Ç–æ–≤: {len(_RECIPES_DB)}. –¢–æ–≤–∞—Ä–æ–≤: {len(_STOCK_DF) if _STOCK_DF is not None else 0}."
        
        if warnings_total:
            msg += f" –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {len(warnings_total)}."
            
        return True, msg
    except Exception as exc:
        LAST_SYNC_META["warnings"] = [str(exc)]
        return False, f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {exc}"
