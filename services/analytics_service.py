import pandas as pd
import numpy as np
from datetime import timedelta
from typing import List, Dict, Any, Tuple, Optional, Union
from use_cases.domain_models import InsightMetric

def calculate_insights(df_curr: pd.DataFrame, df_prev: pd.DataFrame, cur_rev: float, prev_rev: float, cur_fc: float) -> List[InsightMetric]:
    """
    Calculates business insights/alerts based on current and previous data.
    Returns a list of InsightMetric DTOs.
    Levels: 'success', 'warning', 'error', 'info'
    """
    insights = []
    
    # 1. Revenue Check
    if prev_rev > 0:
        rev_diff_pct = (cur_rev - prev_rev) / prev_rev * 100
        if rev_diff_pct < -10:
            insights.append(InsightMetric(
                type='rev_drop',
                message=f"üìâ **–¢—Ä–µ–≤–æ–≥–∞ –ø–æ –í—ã—Ä—É—á–∫–µ**: –ü–∞–¥–µ–Ω–∏–µ –Ω–∞ {abs(rev_diff_pct):.1f}% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ—à–ª—ã–º –ø–µ—Ä–∏–æ–¥–æ–º.",
                level='error'
            ))
        elif rev_diff_pct > 20:
            insights.append(InsightMetric(
                type='rev_growth',
                message=f"üöÄ **–û—Ç–ª–∏—á–Ω—ã–π —Ä–æ—Å—Ç**: –í—ã—Ä—É—á–∫–∞ –≤—ã—Ä–æ—Å–ª–∞ –Ω–∞ {rev_diff_pct:.1f}%!",
                level='success'
            ))

    # 2. Food Cost Check
    TARGET_FC = 35.0
    if cur_fc > TARGET_FC:
        insights.append(InsightMetric(
            type='high_fc',
            message=f"‚ö†Ô∏è **–í—ã—Å–æ–∫–∏–π –§—É–¥-–∫–æ—Å—Ç**: –¢–µ–∫—É—â–∏–π {cur_fc:.1f}% (–¶–µ–ª—å: {TARGET_FC}%).",
            level='warning'
        ))
    
    # 3. Ingredient Inflation (Top Spike)
    if not df_prev.empty and 'Unit_Cost' in df_curr.columns and 'Unit_Cost' in df_prev.columns:
        # Compare average purchase prices
        curr_prices = df_curr.groupby('–ë–ª—é–¥–æ')['Unit_Cost'].mean()
        prev_prices = df_prev.groupby('–ë–ª—é–¥–æ')['Unit_Cost'].mean()
        
        safe_prev_prices = prev_prices.replace(0, np.nan)
        price_changes = (curr_prices - safe_prev_prices) / safe_prev_prices * 100
        price_changes = price_changes.replace([np.inf, -np.inf], np.nan).dropna().sort_values(ascending=False)
        
        if not price_changes.empty:
            top_inflator = price_changes.index[0]
            top_val = price_changes.iloc[0]
            if top_val > 15: # Raised/Spiked more than 15%
                insights.append(InsightMetric(
                    type='inflation',
                    message=f"üí∏ **–°–∫–∞—á–æ–∫ —Ü–µ–Ω—ã**: {top_inflator} –ø–æ–¥–æ—Ä–æ–∂–∞–ª –Ω–∞ {top_val:.0f}%.",
                    level='warning'
                ))

    # 4. Dead Items ("Dogs")
    # Logic: Low Sales (< Avg) AND Low Margin (< Avg)
    if not df_curr.empty:
        item_stats = df_curr.groupby('–ë–ª—é–¥–æ').agg({'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': 'sum', '–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°': 'sum', '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å': 'sum'}).reset_index()
        item_stats['–ú–∞—Ä–∂–∞'] = item_stats['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'] - item_stats['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å']
        item_stats = item_stats[item_stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] > 0]
        
        if not item_stats.empty:
            avg_qty = item_stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'].mean()
            avg_margin = item_stats['–ú–∞—Ä–∂–∞'].mean()
            
            dogs = item_stats[(item_stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] < avg_qty * 0.5) & (item_stats['–ú–∞—Ä–∂–∞'] < avg_margin * 0.5)]
            if len(dogs) > 5:
                insights.append(InsightMetric(
                    type='dogs',
                    message=f"üê∂ **–ú–µ—Ä—Ç–≤—ã–π –≥—Ä—É–∑**: –ù–∞–π–¥–µ–Ω–æ {len(dogs)} –ø–æ–∑–∏—Ü–∏–π '–°–æ–±–∞–∫' (–º–∞–ª–æ –ø—Ä–æ–¥–∞–∂, –º–∞–ª–æ –¥–µ–Ω–µ–≥).",
                    level='info'
                ))

    if not insights:
        insights.append(InsightMetric(
            type='all_good',
            message="‚úÖ **–í—Å—ë —Å–ø–æ–∫–æ–π–Ω–æ**: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.",
            level='success'
        ))

    return insights

def compute_inflation_metrics(df_scope: pd.DataFrame, df_v: pd.DataFrame) -> Tuple[float, float, pd.DataFrame]:
    if df_scope.empty or df_v.empty:
        return 0, 0, pd.DataFrame()
    last_prices = df_scope.sort_values('–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞').groupby('–ë–ª—é–¥–æ')['Unit_Cost'].last()
    current_prices = df_v.groupby('–ë–ª—é–¥–æ')['Unit_Cost'].mean()

    merged = pd.concat([last_prices, current_prices], axis=1, keys=['Old', 'New']).dropna()
    merged['Diff'] = merged['New'] - merged['Old']
    merged['Pct'] = (merged['Diff'] / merged['Old']) * 100

    qty_map = df_v.groupby('–ë–ª—é–¥–æ')['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'].sum()
    merged['Qty'] = qty_map
    merged['Effect'] = merged['Diff'] * merged['Qty']

    loss = merged[merged['Effect'] > 0]['Effect'].sum()
    save = abs(merged[merged['Effect'] < 0]['Effect'].sum())

    detail = merged[merged['Effect'] != 0].copy()
    detail['–¢–æ–≤–∞—Ä'] = detail.index
    detail['–†–æ—Å—Ç %'] = detail['Pct']
    detail['–≠—Ñ—Ñ–µ–∫—Ç (‚ÇΩ)'] = detail['Effect']
    return loss, save, detail


def compute_supplier_stats(df: pd.DataFrame) -> pd.DataFrame:
    if '–ü–æ—Å—Ç–∞–≤—â–∏–∫' not in df.columns or df.empty:
        return pd.DataFrame()
    return (
        df.groupby('–ü–æ—Å—Ç–∞–≤—â–∏–∫')['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å']
        .sum()
        .reset_index()
        .sort_values('–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å', ascending=False)
        .head(15)
    )


def compute_menu_tab_data(df: pd.DataFrame, group_col: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if df.empty:
        return pd.DataFrame(), pd.DataFrame()
    cat_df = (
        df.groupby(group_col)['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°']
        .sum()
        .reset_index()
        .sort_values(by='–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°', ascending=False)
    )

    menu_df = df.groupby('–ë–ª—é–¥–æ').agg({
        '–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°': 'sum',
        '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å': 'sum',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': 'sum'
    }).reset_index()
    menu_df['–§—É–¥–∫–æ—Å—Ç %'] = (menu_df['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å'] / menu_df['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'] * 100).fillna(0)
    menu_df = menu_df.sort_values('–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°', ascending=False)
    return cat_df, menu_df


def compute_abc_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, float, float]:
    if df.empty:
        return pd.DataFrame(), 0, 0
    abc = df.groupby('–ë–ª—é–¥–æ').agg({
        '–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°': 'sum',
        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': 'sum',
        '–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å': 'sum'
    }).reset_index()
    abc['Margin'] = abc['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'] - abc['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å']
    abc['Unit_Margin'] = abc['Margin'] / abc['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ']

    avg_qty = abc['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'].mean()
    avg_margin = abc['Unit_Margin'].mean()

    def classify(row):
        high_vol = row['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] >= avg_qty
        high_prof = row['Unit_Margin'] >= avg_margin
        if high_vol and high_prof:
            return "‚≠ê –ó–≤–µ–∑–¥–∞"
        if high_vol and not high_prof:
            return "üêé –õ–æ—à–∞–¥–∫–∞"
        if not high_vol and high_prof:
            return "‚ùì –ó–∞–≥–∞–¥–∫–∞"
        return "üê∂ –°–æ–±–∞–∫–∞"

    abc['–ö–ª–∞—Å—Å'] = abc.apply(classify, axis=1)
    return abc, avg_qty, avg_margin


def compute_weekday_stats(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if df.empty:
        return pd.DataFrame(), pd.DataFrame()

    ru_days = {
        0: '–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', 1: '–í—Ç–æ—Ä–Ω–∏–∫', 2: '–°—Ä–µ–¥–∞', 3: '–ß–µ—Ç–≤–µ—Ä–≥',
        4: '–ü—è—Ç–Ω–∏—Ü–∞', 5: '–°—É–±–±–æ—Ç–∞', 6: '–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ'
    }

    daily = df.groupby('–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞')['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'].sum().reset_index()
    daily['–î–µ–Ω—å–†—É—Å'] = daily['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'].dt.weekday.map(ru_days)
    daily['–î–∞—Ç–∞_–ü–æ–¥–ø–∏—Å—å'] = daily['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'].dt.strftime('%d.%m')

    dates_per_weekday = df[['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞']].drop_duplicates()
    dates_per_weekday['Day'] = dates_per_weekday['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'].dt.weekday.map(ru_days)
    counts = dates_per_weekday['Day'].value_counts()

    sums = df.groupby(df['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'].dt.weekday.map(ru_days))['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'].sum()
    avgs = (sums / counts).rename('–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°').rename_axis('–î–µ–Ω—å–†—É—Å').reset_index()

    days_order = {
        '–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫': 0, '–í—Ç–æ—Ä–Ω–∏–∫': 1, '–°—Ä–µ–¥–∞': 2, '–ß–µ—Ç–≤–µ—Ä–≥': 3,
        '–ü—è—Ç–Ω–∏—Ü–∞': 4, '–°—É–±–±–æ—Ç–∞': 5, '–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ': 6
    }
    avgs['SortKey'] = avgs['–î–µ–Ω—å–†—É—Å'].map(days_order)
    avgs = avgs.sort_values('SortKey').drop(columns=['SortKey'])

    return daily, avgs


def compute_purchase_plan(df: pd.DataFrame, days: int, safety: int) -> pd.DataFrame:
    if df.empty:
        return pd.DataFrame(columns=['Budget'])
    end_dt = df['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'].max()
    start_dt = end_dt - timedelta(days=30)
    recent = df[df['–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞'] >= start_dt]

    daily_usage = recent.groupby('–ë–ª—é–¥–æ')['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'].sum() / 30
    last_cost = recent.sort_values('–î–∞—Ç–∞_–û—Ç—á–µ—Ç–∞').groupby('–ë–ª—é–¥–æ')['Unit_Cost'].last()

    plan = pd.DataFrame({'Daily_Use': daily_usage, 'Unit_Cost': last_cost}).dropna()
    plan['Need_Qty'] = plan['Daily_Use'] * days * (1 + safety/100)
    plan['Budget'] = plan['Need_Qty'] * plan['Unit_Cost']

    return plan.sort_values('Budget', ascending=False).reset_index()


def compute_simulation(df: pd.DataFrame, cats: List[str], d_price: float, d_cost: float, d_vol: float) -> Optional[Dict[str, float]]:
    if df.empty:
        return None
    mask = df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].isin(cats)
    target = df[mask].copy()
    other = df[~mask].copy()

    base_rev = df['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'].sum()
    base_cost = df['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å'].sum()
    base_margin = base_rev - base_cost

    sim_rev_target = target['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'].sum() * (1 + d_price/100) * (1 + d_vol/100)
    sim_cost_target = target['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å'].sum() * (1 + d_cost/100) * (1 + d_vol/100)

    sim_rev = other['–í—ã—Ä—É—á–∫–∞ —Å –ù–î–°'].sum() + sim_rev_target
    sim_cost = other['–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å'].sum() + sim_cost_target
    sim_margin = sim_rev - sim_cost

    return {
        'base_revenue': base_rev,
        'base_margin': base_margin,
        'sim_revenue': sim_rev,
        'sim_margin': sim_margin,
        'diff_rev': sim_rev - base_rev,
        'diff_margin': sim_margin - base_margin,
        'old_profitability': (base_margin / base_rev * 100) if base_rev else 0,
        'new_profitability': (sim_margin / sim_rev * 100) if sim_rev else 0
    }

def get_unique_ingredients(recipes_db: Dict[str, List[Dict[str, Any]]]) -> List[str]:
    """
    Extracts a sorted list of unique ingredient names from the recipes database.
    """
    ingredients = set()
    for dish_ingredients in recipes_db.values():
        for ing in dish_ingredients:
            if ing.get('ingredient'):
                ingredients.add(ing['ingredient'])
    return sorted(list(ingredients))

def simulate_forecast(
    recipes_db: Dict[str, List[Dict[str, Any]]],
    ingredient_deltas: Dict[str, float],
    df_current: pd.DataFrame
) -> pd.DataFrame:
    """
    Calculates the impact of ingredient price changes on dish unit costs.
    
    Args:
        recipes_db: Dictionary of recipes {dish_name: [{'ingredient': name, 'qty_per_dish': qty}, ...]}
        ingredient_deltas: Dictionary of price increases per unit {ingredient_name: price_increase_rub}
        df_current: DataFrame containing current dish data (must have '–ë–ª—é–¥–æ', 'Unit_Cost', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
        
    Returns:
        DataFrame with columns: ['–ë–ª—é–¥–æ', '–¢–µ–∫—É—â–∞—è —Å/—Å', '–†–æ—Å—Ç —Å/—Å', '–ù–æ–≤–∞—è —Å/—Å', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ']
    """
    if not recipes_db or not ingredient_deltas or df_current.empty:
        return pd.DataFrame()

    dish_impacts = {}

    # 1. Calculate impact per dish based on recipes
    for dish_name, ingredients in recipes_db.items():
        # Recipe dish names are typically normalized (lowercase) by parsing_service
        # But we ensure it here just in case
        norm_dish = str(dish_name).lower().strip()
        
        impact = 0.0
        for ing in ingredients:
            ing_name = ing.get('ingredient') # normalized by parsing_service
            qty = ing.get('qty_per_dish', 0)
            
            # check if this ingredient has a price increase
            if ing_name in ingredient_deltas:
                delta = ingredient_deltas[ing_name]
                impact += qty * delta
        
        if impact > 0:
            dish_impacts[norm_dish] = impact

    if not dish_impacts:
        return pd.DataFrame()

    # 2. Map impact to existing dishes in DataFrame
    # Create a normalized lookup column for sales data
    df_work = df_current.copy()
    df_work['dish_norm'] = df_work['–ë–ª—é–¥–æ'].astype(str).str.lower().str.strip()
    
    # Filter only relevant dishes using normalized names
    affected_mask = df_work['dish_norm'].isin(dish_impacts.keys())
    affected_dishes = df_work[affected_mask].copy()
    
    if affected_dishes.empty:
        return pd.DataFrame()
        
    results = []
    
    for _, row in affected_dishes.iterrows():
        dish_display = row['–ë–ª—é–¥–æ']
        dish_norm = row['dish_norm']
        
        current_cost = row.get('Unit_Cost', 0)
        # Look up impact using normalized key
        impact = dish_impacts.get(dish_norm, 0)
        
        new_cost = current_cost + impact
        qty = row.get('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', 0)
        
        results.append({
            '–ë–ª—é–¥–æ': dish_display,
            '–¢–µ–∫—É—â–∞—è —Å/—Å': current_cost,
            '–†–æ—Å—Ç —Å/—Å': impact,
            '–ù–æ–≤–∞—è —Å/—Å': new_cost,
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': qty
        })
        
    return pd.DataFrame(results)
